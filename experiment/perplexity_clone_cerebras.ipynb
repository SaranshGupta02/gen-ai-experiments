{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Build Fast with AI](https://img.shields.io/badge/BuildFastWithAI-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://www.buildfastwithai.com/genai-course)\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1cYJcJ42zuPa8O0L-pFZiZzPlehtl7UGo#scrollTo=zWo55QTF-mJ2)\n"
      ],
      "metadata": {
        "id": "UFwfv5vCCQkO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîç Perplexity Clone using LangChain + Cerebras + Tavily Search\n",
        "\n",
        "In this notebook, we will build a Perplexity-like conversational search assistant that:\n",
        "- Uses Tavily API to search the web,\n",
        "- Summarizes results using a Cerebras LLM,\n",
        "- Maintains chat history using LangChain's RunnableWithMessageHistory.\n"
      ],
      "metadata": {
        "id": "MeOqvuuQ92j-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install langchain langchain-community langchain-cerebras tavily-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkuPsX5f926u",
        "outputId": "8738d07f-f726-41cd-8f66-8725ae678ca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-cerebras\n",
            "  Downloading langchain_cerebras-0.5.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting tavily-python\n",
            "  Downloading tavily_python-0.7.12-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.79)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.37)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.44)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting requests<3,>=2 (from langchain)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.11.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Collecting langchain-openai<0.4.0,>=0.3.0 (from langchain-cerebras)\n",
            "  Downloading langchain_openai-0.3.35-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: tiktoken>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tavily-python) (0.12.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from tavily-python) (0.28.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain-openai<0.4.0,>=0.3.0->langchain-cerebras) (1.109.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx->tavily-python) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx->tavily-python) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->tavily-python) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx->tavily-python) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->tavily-python) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.5.1->tavily-python) (2024.11.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai<0.4.0,>=0.3.0->langchain-cerebras) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai<0.4.0,>=0.3.0->langchain-cerebras) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai<0.4.0,>=0.3.0->langchain-cerebras) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai<0.4.0,>=0.3.0->langchain-cerebras) (4.67.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.3.31-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_cerebras-0.5.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading tavily_python-0.7.12-py3-none-any.whl (15 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_openai-0.3.35-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, mypy-extensions, marshmallow, typing-inspect, tavily-python, dataclasses-json, langchain-openai, langchain-cerebras, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-cerebras-0.5.0 langchain-community-0.3.31 langchain-openai-0.3.35 marshmallow-3.26.1 mypy-extensions-1.1.0 requests-2.32.5 tavily-python-0.7.12 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîë Setup API Keys\n",
        "\n",
        "We will use:\n",
        "- `TAVILY_API_KEY` for Tavily Search\n",
        "- `CEREBRAS_API_KEY` for the Cerebras LLM\n",
        "\n",
        "Make sure to get both keys and add them to your environment.\n"
      ],
      "metadata": {
        "id": "1v3hYtDJ9-RU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "# Set your API keys (you can also use environment variables in Colab or .env)\n",
        "api_key = userdata.get(\"TAVILY_API_KEY\").strip()\n",
        "\n",
        "os.environ[\"CEREBRAS_API_KEY\"] = userdata.get(\"CEREBRAS_API_KEY\")"
      ],
      "metadata": {
        "id": "OMbuSw1998lI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß© Import libraries and initialize tools\n",
        "We‚Äôll use:\n",
        "- `TavilySearchResults` for fetching relevant snippets.\n",
        "- `ChatCerebras` for generating answers.\n",
        "- `RunnableWithMessageHistory` to maintain chat memory.\n"
      ],
      "metadata": {
        "id": "8mqypX2o-gC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tavily import TavilyClient\n",
        "from langchain_cerebras import ChatCerebras\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "\n",
        "client = TavilyClient(api_key)\n",
        "# response = client.search(\n",
        "#     query=\"what is genai\"\n",
        "# )  check\n",
        "\n",
        "# print(response)\n",
        "# Initialize Cerebras model\n",
        "llm = ChatCerebras(model=\"qwen-3-32b\", temperature=0.4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsBtPU1v-NAc",
        "outputId": "95b13105-4894-44c7-d69f-428bd018024e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "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\r\n",
            "\n",
            "{'query': 'what is genai', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://libguides.acu.edu.au/ai-basics', 'title': 'What is GenAI - AI basics - Library guides', 'content': '**Generative AI (GenAI)** is a specialised form that creates new content- text, images, video, audio, or code, based on prompts and patterns in existing data. There are many GenAI tools out there to choose from, and they can generate different types of content. * **Text content:** AI tools that generate text responses are trained on a massive amount of data. * **Sound and video:** Sound and video based GenAI tools can generate content such as music compositions in a range of genres, sound effects for movies and games or human-looking avatars. * **Research discovery:** Research discovery based GenAI tools can automate parts of the research process and make long, complex texts easier to decipher by extracting key information or summarising a paper.', 'score': 0.9281033, 'raw_content': None}, {'url': 'https://canberra.libguides.com/c.php?g=970043&p=7053078', 'title': 'What is Gen AI? - GenAI for Students - UC Library Guides - LibGuides', 'content': 'Generative Artificial Intelligence, or GenAI, is a type of artificial intelligence that allows machines to create new content like text, images, music, videos, code, and more, based on inputs or prompts. When you use a GenAI tool, it\\xa0analyses these data sets, and generates new content that resembles what it has learned, much like the predictive text on your phone. | * Brainstorm and idea generation * Text translation * Create practice quizzes and questions * Summarise difficult texts | * Plagiarism risk * Has been known to make up content * Correct citation and referencing needed * Academic integrity is essential - all use must be \"permitted and admitted\" | Microsoft Copilot  ChatGPT  Perplexity AI | | Video GenAI | Uses images and text to generate video content.', 'score': 0.9277898, 'raw_content': None}, {'url': 'https://genai.umich.edu/about-generative-ai', 'title': 'About Generative Artificial Intelligence - UM-GPT', 'content': 'Generative AI (GenAI) is a general term for artificial intelligence that creates new content by generating new data samples that are similar to the training', 'score': 0.9210814, 'raw_content': None}, {'url': 'https://www.snaplogic.com/glossary/what-is-genai', 'title': 'What is Generative AI (GenAI)? - SnapLogic', 'content': '**TLDR**: Generative AI (GenAI) is a type of artificial intelligence that learns from large amounts of data and then creates new content such as text, images, code, or music. At SnapLogic, these capabilities are built directly into products such as **SnapLogic AI**, **AgentCreator**, and **SnapGPT (Integration Copilot)**. Generative AI starts with data. * **Customer Support**: Use AI assistants to handle FAQs, triage requests, and provide quick answers, leaving complex cases for human agents. End manual integration and unlock agentic automation Discover how leading enterprises are transforming their operations with AI-powered integration. Integrate data and tools for new AI initiatives SnapLogic integrates AI, data, apps, and microservices into one platform, accelerating AI‚Ä¶', 'score': 0.89997756, 'raw_content': None}, {'url': 'https://genai.byu.edu/understanding-ai', 'title': 'About GenAI', 'content': '# About GenAI ## What is GenAI? Learn how to do each of these things using generative AI in our GenAI training library. Generative AI, or GenAI, operates through a model that contains a series of complex algorithms and vast sets of data. GenAI services create computer code by learning patterns from existing examples. **LLM**: Short for Large Language Model, this refers to a savvy AI model that uses an extensive amount of data, much like reading a vast number of books, to understand and generate text in a human-like manner. **GenAI**: This is an AI variety that can craft new content such as stories, images, sounds, or other creative outputs by identifying patterns in previously processed data.', 'score': 0.8969848, 'raw_content': None}], 'response_time': 1.06, 'request_id': '3937d587-00e9-45d3-b1bd-53074ddf29cd'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Define the system prompt\n",
        "This defines how the model will use search results to answer user questions.\n"
      ],
      "metadata": {
        "id": "TgP2_ukI-jEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"You are an intelligent search assistant. Use the provided web results to answer the question. \"\n",
        "     \"If the results are insufficient, say you couldn‚Äôt find enough information. Always cite sources.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"human\", \"User query: {query}\\n\\nWeb results:\\n{context}\\n\\nAnswer:\")\n",
        "])\n"
      ],
      "metadata": {
        "id": "pQoTgK0G-h4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üåê Define function to perform search + combine with LLM\n",
        "This chain:\n",
        "1. Performs Tavily search\n",
        "2. Builds a combined context\n",
        "3. Passes it to the Cerebras LLM\n"
      ],
      "metadata": {
        "id": "8Yq5bcJ9-mDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_search_and_answer(query: str, history):\n",
        "    # Perform search\n",
        "    results = client.search(query=query)\n",
        "\n",
        "    # If Tavily returns a string, use it directly\n",
        "    if isinstance(results, str):\n",
        "        context = results\n",
        "    # If Tavily returns a list of dicts, format them\n",
        "    elif isinstance(results, list):\n",
        "        context = \"\\n\".join([\n",
        "            f\"{i+1}. {r.get('content', '')} (Source: {r.get('url', 'N/A')})\"\n",
        "            for i, r in enumerate(results)\n",
        "        ])\n",
        "    else:\n",
        "        context = str(results)\n",
        "\n",
        "    # Build the prompt input\n",
        "    prompt_input = {\"query\": query, \"context\": context, \"history\": history}\n",
        "\n",
        "    # Generate response\n",
        "    chain = template | llm\n",
        "    response = chain.invoke(prompt_input)\n",
        "\n",
        "    return response.content, context\n"
      ],
      "metadata": {
        "id": "zWo55QTF-mJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üí¨ Add memory using RunnableWithMessageHistory\n",
        "We‚Äôll use LangChain‚Äôs memory feature to persist conversation context.\n"
      ],
      "metadata": {
        "id": "-xhHxkGW-mP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda, RunnableMap\n",
        "\n",
        "store = {}  # session-wise memory store\n",
        "\n",
        "def get_history(session_id: str) -> InMemoryChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = InMemoryChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "# Step 1: Create Runnable for web search\n",
        "search_runnable = RunnableLambda(lambda x: search.invoke({\"query\": x[\"query\"]}))\n",
        "\n",
        "# Step 2: Combine search + context + query for LLM\n",
        "def prepare_input(inputs):\n",
        "    results = inputs[\"search\"]\n",
        "    context = \"\\n\".join([f\"{i+1}. {r['content']} (Source: {r['url']})\" for i, r in enumerate(results)])\n",
        "    return {\"query\": inputs[\"query\"], \"context\": context, \"history\": inputs[\"history\"]}\n",
        "\n",
        "# Step 3: Define the full chain\n",
        "chain = (\n",
        "    RunnableMap({\n",
        "        \"search\": search_runnable,\n",
        "        \"query\": lambda x: x[\"query\"],\n",
        "        \"history\": lambda x: x[\"history\"],\n",
        "    })\n",
        "    | RunnableLambda(prepare_input)\n",
        "    | template\n",
        "    | llm\n",
        ")\n",
        "\n",
        "# Step 4: Wrap with message history\n",
        "chain_with_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "URzius3T-mVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Test the assistant\n",
        "Now let‚Äôs interact with our Perplexity Clone in this notebook.\n"
      ],
      "metadata": {
        "id": "WPMfWfms-mbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "session_id = \"demo_session\"\n",
        "\n",
        "while True:\n",
        "    user_query = input(\"üßë‚Äçüíª You: \")\n",
        "    if user_query.lower() in [\"exit\", \"quit\"]:\n",
        "        break\n",
        "\n",
        "    history = get_history(session_id)\n",
        "    response, context = run_search_and_answer(user_query, history.messages)\n",
        "\n",
        "    # Save to memory\n",
        "    history.add_message(HumanMessage(content=user_query))\n",
        "    history.add_message(AIMessage(content=response))\n",
        "\n",
        "    print(\"\\nü§ñ AI:\", response, \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YwuLVdlu-mgv",
        "outputId": "f3fb214b-597c-48de-b082-7e13b83e82c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üßë‚Äçüíª You: what is genai\n",
            "\n",
            "ü§ñ AI: <think>\n",
            "Okay, the user is asking \"what is genai\". Let me look at the web results provided.\n",
            "\n",
            "First, the top result from Acu.edu.au defines GenAI as a specialized form of AI that creates new content like text, images, video, audio, or code based on prompts and existing data. It mentions different types of content it can generate, like text, sound, video, and research tools. The score here is high, 0.928, so this seems reliable.\n",
            "\n",
            "The second result from UC Library Guides says GenAI is a type of AI that creates new content such as text, images, music, videos, code. It compares it to predictive text on phones and lists uses like brainstorming, translation, quizzes, and summarizing texts. Also notes risks like plagiarism and made-up content. Score is 0.927, so also trustworthy.\n",
            "\n",
            "Third, the University of Michigan's page defines GenAI as creating new content by generating data samples similar to training data. A bit more general, but aligns with the other definitions.\n",
            "\n",
            "SnapLogic's glossary entry says GenAI is AI that learns from data to create new content like text, images, code, music. They mention applications in customer support and integration platforms. Score is 0.899, still a good source.\n",
            "\n",
            "BYU's page explains GenAI uses complex algorithms and data to create content, mentioning LLMs and examples like stories, images, sounds. Score 0.896.\n",
            "\n",
            "Putting this together, all sources agree that GenAI is a subset of AI focused on generating new content across various media. They mention training on large datasets and applications in different fields. The first two sources from academic libraries provide the most comprehensive explanations, so I should prioritize those. Also, the key points are the types of content generated, the use of prompts, and the underlying training data. Need to mention the potential risks as well, like plagiarism and fabricated content, as noted in the UC guide. Make sure to cite the sources properly as per the instructions.\n",
            "</think>\n",
            "\n",
            "Generative Artificial Intelligence (GenAI) is a subset of artificial intelligence designed to create new content by analyzing patterns in existing data and generating outputs such as **text, images, video, audio, or code** based on user prompts. Key aspects include:\n",
            "\n",
            "1. **Content Creation**:  \n",
            "   - **Text**: Tools like ChatGPT or Google Bard generate written content, summaries, translations, or code by learning from vast datasets.  \n",
            "   - **Multimedia**: GenAI can produce music, sound effects, videos, or synthetic avatars (e.g., AI-generated video tools).  \n",
            "   - **Research Assistance**: Automates tasks like summarizing complex texts or extracting key information from papers.  \n",
            "\n",
            "2. **Mechanism**:  \n",
            "   GenAI models, such as Large Language Models (LLMs), use extensive training data to predict and generate human-like outputs. For example, they function similarly to predictive text but at a far more advanced scale (UC Library Guides, SnapLogic).  \n",
            "\n",
            "3. **Applications**:  \n",
            "   - Idea brainstorming, practice quiz creation, and simplifying difficult texts.  \n",
            "   - Customer support via AI assistants handling FAQs or triaging requests.  \n",
            "   - Automating parts of creative or research workflows.  \n",
            "\n",
            "4. **Risks and Considerations**:  \n",
            "   - Potential for plagiarism or generating factually incorrect content (\"hallucinations\").  \n",
            "   - Requires proper citation and adherence to academic/integrity standards (UC Library Guides).  \n",
            "\n",
            "For more details, refer to [Acu.edu.au](https://libguides.acu.edu.au/ai-basics) or [UC Library Guides](https://canberra.libguides.com/c.php?g=970043&p=7053078). \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3566194015.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0muser_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üßë‚Äçüíª You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xmDob2eq-mmC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xgR1CDjj-mrH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}