{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header_cell"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
        "\n",
        "[![Gen AI Experiments](https://img.shields.io/badge/Gen%20AI%20Experiments-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://github.com/buildfastwithai/gen-ai-experiments)\n",
        "[![Gen AI Experiments GitHub](https://img.shields.io/github/stars/buildfastwithai/gen-ai-experiments?style=for-the-badge&logo=github&color=gold)](http://github.com/buildfastwithai/gen-ai-experiments)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1u10wrxIRiRcZTLO_0_85UafCP7yO99pp?usp=sharing)\n",
        "\n",
        "## Master Generative AI in 8 Weeks\n",
        "**What You'll Learn:**\n",
        "- Master cutting-edge AI tools & frameworks\n",
        "- 6 weeks of hands-on, project-based learning\n",
        "- Weekly live mentorship sessions\n",
        "- Join Innovation Community\n",
        "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
        "[Start Your Journey](https://www.buildfastwithai.com/genai-course)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_cell"
      },
      "source": [
        "# Testing Claude 4.5 Haiku Model Using OpenRouter\n",
        "\n",
        "This notebook provides a comprehensive guide to using the Claude 4.5 Haiku model via OpenRouter's API within the LangChain framework. We will cover everything from basic setup to advanced examples including agent workflows, tool calling, and search integration.\n",
        "\n",
        "**Note:** You will need an API key from [OpenRouter](https://openrouter.ai/) to run the examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Nqrbizb1zzj"
      },
      "source": [
        "### Installation\n",
        "\n",
        "First, let's install the necessary Python libraries for Claude 4.5 Haiku testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ju1pys711zzj"
      },
      "outputs": [],
      "source": [
        "%pip install langchain langchain-openai langchain-community tavily-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDxPLKse1zzk"
      },
      "source": [
        "### Basic Usage with ChatOpenAI and OpenRouter\n",
        "\n",
        "Here's how to set up the `ChatOpenAI` class to connect to the Claude 4.5 Haiku model through OpenRouter.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHy7xS6U1zzk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# It's recommended to set your API key as an environment variable\n",
        "api_key = userdata.get(\"OPENROUTER_API_KEY\")\n",
        "\n",
        "# Initialize the ChatOpenAI model for Claude 4.5 Haiku\n",
        "claude_llm = ChatOpenAI(\n",
        "    model=\"anthropic/claude-haiku-4.5\",\n",
        "    openai_api_key=api_key,\n",
        "    openai_api_base=\"https://openrouter.ai/api/v1\"\n",
        ")\n",
        "\n",
        "# Let's test it with a simple prompt\n",
        "response = claude_llm.invoke(\"What is the Model Context Protocol?\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc5t95r31zzk"
      },
      "source": [
        "### Multilingual Capabilities\n",
        "\n",
        "Claude 4.5 Haiku is proficient in multiple languages. Let's test this by sending prompts in Hindi and Spanish.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-cb2roa1zzk"
      },
      "outputs": [],
      "source": [
        "# Example in Hindi\n",
        "hindi_prompt = \"कृत्रिम बुद्धिमत्ता क्या है?\" # Translation: What is Artificial Intelligence?\n",
        "hindi_response = claude_llm.invoke(hindi_prompt)\n",
        "print(f\"Response in Hindi:\\n{hindi_response.content}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Example in Spanish\n",
        "spanish_prompt = \"¿Cuál es la capital de Argentina?\" # Translation: What is the capital of Argentina?\n",
        "spanish_response = claude_llm.invoke(spanish_prompt)\n",
        "print(f\"Response in Spanish:\\n{spanish_response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHT_H2ZK1zzl"
      },
      "source": [
        "### Building a Simple Chatbot\n",
        "\n",
        "We can create a simple conversational chatbot by managing the chat history.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDh3w1n91zzl"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant that speaks like a pirate.\"),\n",
        "    HumanMessage(content=\"Ahoy! What's your name?\"),\n",
        "]\n",
        "\n",
        "# First turn\n",
        "response = claude_llm.invoke(messages)\n",
        "print(f\"Pirate Bot: {response.content}\")\n",
        "\n",
        "# Add the bot's response to the history\n",
        "messages.append(response)\n",
        "\n",
        "# Second turn\n",
        "messages.append(HumanMessage(content=\"What's the weather like today?\"))\n",
        "response = claude_llm.invoke(messages)\n",
        "print(f\"\\nPirate Bot: {response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfE0k6aO1zzl"
      },
      "source": [
        "## Advanced Examples\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOeCJoFg1zzl"
      },
      "source": [
        "### Tool Calling and Tavily Search Using Claude 4.5 Haiku\n",
        "\n",
        "Let's create an agent that can use tools, specifically the Tavily search tool for real-time information retrieval.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgRNk3WO1zzl"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_community.tools import TavilySearchResults\n",
        "import os\n",
        "\n",
        "# 1. Setup LLM via OpenRouter\n",
        "llm = ChatOpenAI(\n",
        "    model=\"anthropic/claude-haiku-4.5\",\n",
        "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
        "    openai_api_key=api_key,\n",
        ")\n",
        "\n",
        "# 2. Setup Tavily Search Tool\n",
        "# Note: You'll need a Tavily API key - get one at https://tavily.com/\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_KEY\")\n",
        "tavily_tool = TavilySearchResults(max_results=2)\n",
        "\n",
        "# 3. Create Agent\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI assistant with access to search tools.\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "])\n",
        "agent = create_tool_calling_agent(llm, [tavily_tool], prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=[tavily_tool], verbose=True)\n",
        "\n",
        "# 4. Run Agent\n",
        "response = agent_executor.invoke({\"input\": \"What are the latest developments in AI safety research in 2024?\"})\n",
        "print(response['output'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHctan3X1zzm"
      },
      "source": [
        "### Custom Tool Creation and Agent Workflow\n",
        "\n",
        "Let's create a custom tool and demonstrate more advanced agent capabilities.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWd9k_AT1zzm"
      },
      "outputs": [],
      "source": [
        "from langchain.tools import tool\n",
        "import math\n",
        "\n",
        "@tool\n",
        "def calculate_area(shape: str, **kwargs) -> str:\n",
        "    \"\"\"Calculate the area of different shapes.\n",
        "\n",
        "    Args:\n",
        "        shape: The shape to calculate area for (circle, rectangle, triangle)\n",
        "        **kwargs: Shape-specific parameters\n",
        "            - For circle: radius\n",
        "            - For rectangle: length, width\n",
        "            - For triangle: base, height\n",
        "    \"\"\"\n",
        "    shape = shape.lower()\n",
        "\n",
        "    if shape == \"circle\":\n",
        "        radius = kwargs.get(\"radius\", 0)\n",
        "        area = math.pi * radius ** 2\n",
        "        return f\"The area of a circle with radius {radius} is {area:.2f} square units.\"\n",
        "\n",
        "    elif shape == \"rectangle\":\n",
        "        length = kwargs.get(\"length\", 0)\n",
        "        width = kwargs.get(\"width\", 0)\n",
        "        area = length * width\n",
        "        return f\"The area of a rectangle with length {length} and width {width} is {area} square units.\"\n",
        "\n",
        "    elif shape == \"triangle\":\n",
        "        base = kwargs.get(\"base\", 0)\n",
        "        height = kwargs.get(\"height\", 0)\n",
        "        area = 0.5 * base * height\n",
        "        return f\"The area of a triangle with base {base} and height {height} is {area} square units.\"\n",
        "\n",
        "    else:\n",
        "        return f\"Sorry, I don't know how to calculate the area of a {shape}.\"\n",
        "\n",
        "# Create agent with multiple tools\n",
        "tools = [tavily_tool, calculate_area]\n",
        "agent = create_tool_calling_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "# Test the agent with multiple tool usage\n",
        "response = agent_executor.invoke({\n",
        "    \"input\": \"Calculate the area of a circle with radius 5, and then search for information about the mathematical constant pi.\"\n",
        "})\n",
        "print(response['output'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSSJN-KQ1zzm"
      },
      "source": [
        "### Advanced Parameter Tuning\n",
        "\n",
        "You can control the model's output by tuning parameters like `temperature` and `max_tokens`.\n",
        "\n",
        "- **`temperature`**: Controls randomness. Lower values (e.g., 0.1) make the output more deterministic, while higher values (e.g., 0.9) make it more creative.\n",
        "- **`max_tokens`**: Sets the maximum length of the generated response.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DE54uvnB1zzm"
      },
      "outputs": [],
      "source": [
        "# Creative response with high temperature\n",
        "creative_llm = ChatOpenAI(\n",
        "    model=\"anthropic/claude-haiku-4.5\",\n",
        "    openai_api_key=api_key,\n",
        "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
        "    temperature=0.9,\n",
        "    max_tokens=150\n",
        ")\n",
        "\n",
        "prompt = \"Write a short, futuristic story about a cat who can code.\"\n",
        "creative_response = creative_llm.invoke(prompt)\n",
        "print(f\"Creative Story:\\n{creative_response.content}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Factual response with low temperature\n",
        "factual_llm = ChatOpenAI(\n",
        "    model=\"anthropic/claude-haiku-4.5\",\n",
        "    openai_api_key=api_key,\n",
        "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "prompt = \"Explain the theory of relativity in simple terms.\"\n",
        "factual_response = factual_llm.invoke(prompt)\n",
        "print(f\"Factual Explanation:\\n{factual_response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXTk_-cc1zzm"
      },
      "source": [
        "### Code Generation\n",
        "\n",
        "Let's test Claude 4.5 Haiku's code generation capabilities.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgtIMRHP1zzm"
      },
      "outputs": [],
      "source": [
        "prompt = \"Write a Python function that takes a list of numbers and returns the sum of all even numbers in the list.\"\n",
        "code_response = claude_llm.invoke(prompt)\n",
        "print(f\"Generated Code:\\n{code_response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEhSNFNX1zzm"
      },
      "source": [
        "### Few-Shot Prompting\n",
        "\n",
        "Few-shot prompting provides the model with examples to guide its response format.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Kg6NEh7K1zzm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88763b39-93be-4bc0-ade2-91ac0da77562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation of 'house': \"house -> maison\"\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Translate the following English words to French:\n",
        "\n",
        "\"sea -> mer\"\n",
        "\"sky -> ciel\"\n",
        "\"book -> livre\"\n",
        "\"house ->\"\n",
        "\"\"\"\n",
        "\n",
        "few_shot_response = claude_llm.invoke(prompt)\n",
        "print(f\"Translation of 'house': {few_shot_response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxJYbsZ11zzn"
      },
      "source": [
        "### Streaming Responses\n",
        "\n",
        "For longer responses, you can use streaming to get real-time output.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLY6nJmw1zzn"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Create a streaming version of the model\n",
        "streaming_llm = ChatOpenAI(\n",
        "    model=\"anthropic/claude-haiku-4.5\",\n",
        "    openai_api_key=api_key,\n",
        "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "prompt = \"Write a detailed explanation of how machine learning works, covering the key concepts.\"\n",
        "\n",
        "print(\"Streaming response:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Stream the response\n",
        "for chunk in streaming_llm.stream(prompt):\n",
        "    print(chunk.content, end=\"\", flush=True)\n",
        "    time.sleep(0.01)  # Small delay to visualize streaming\n",
        "\n",
        "print(\"\\n\" + \"-\" * 50)\n",
        "print(\"✅ Streaming complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Yl6-26U1zzn"
      },
      "source": [
        "### Performance Comparison\n",
        "\n",
        "Let's compare Claude 4.5 Haiku with different temperature settings for various tasks.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX9Gpswc1zzn"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def test_model_performance(prompt, temperatures=[0.1, 0.5, 0.9]):\n",
        "    \"\"\"Test the same prompt with different temperature settings\"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for temp in temperatures:\n",
        "        print(f\"\\n🌡️ Testing with temperature: {temp}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        test_llm = ChatOpenAI(\n",
        "            model=\"anthropic/claude-haiku-4.5\",\n",
        "            openai_api_key=api_key,\n",
        "            openai_api_base=\"https://openrouter.ai/api/v1\",\n",
        "            temperature=temp,\n",
        "            max_tokens=100\n",
        "        )\n",
        "\n",
        "        start_time = time.time()\n",
        "        response = test_llm.invoke(prompt)\n",
        "        end_time = time.time()\n",
        "\n",
        "        results[temp] = {\n",
        "            'response': response.content,\n",
        "            'time': end_time - start_time\n",
        "        }\n",
        "\n",
        "        print(f\"Response: {response.content}\")\n",
        "        print(f\"⏱️ Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Test with a creative prompt\n",
        "creative_prompt = \"Write a haiku about artificial intelligence.\"\n",
        "print(\"🎨 CREATIVE TASK: Writing a haiku about AI\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "creative_results = test_model_performance(creative_prompt)\n",
        "\n",
        "print(\"\\n\\n📊 ANALYTICAL TASK: Explaining a concept\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Test with an analytical prompt\n",
        "analytical_prompt = \"Explain the concept of recursion in programming.\"\n",
        "analytical_results = test_model_performance(analytical_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wUvwUmz1zzn"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated the capabilities of Claude 4.5 Haiku through OpenRouter, including:\n",
        "\n",
        "1. **Basic Setup**: How to configure and initialize the model\n",
        "2. **Simple Chat**: Basic conversational interactions\n",
        "3. **Multilingual Support**: Testing with Hindi and Spanish\n",
        "4. **Agent Workflows**: Creating agents with tool calling capabilities\n",
        "5. **Tavily Integration**: Real-time search functionality\n",
        "6. **Custom Tools**: Building and integrating custom tools\n",
        "7. **Parameter Tuning**: Controlling creativity vs. factual accuracy\n",
        "8. **Code Generation**: Programming task assistance\n",
        "9. **Few-Shot Learning**: Pattern-based responses\n",
        "10. **Structured Output**: JSON generation\n",
        "11. **Streaming**: Real-time response generation\n",
        "12. **Performance Testing**: Comparing different configurations\n",
        "\n",
        "Claude 4.5 Haiku offers excellent performance for a wide range of tasks while being cost-effective and fast. It's particularly well-suited for applications requiring quick responses and efficient processing.\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}